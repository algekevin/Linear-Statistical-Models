---
title: "Coding Challenge 1 Submission"
author: "Kevin Bailey, Yuying(Bella) Guan"
date: "March 27, 2019"
output: html_document
---
Loading libraries.
```{r}
library(boot)   # For CV
library(leaps)  # For regsubsets()
library(glmnet) # For Lasso
library(splines)# For Splines
library(fields) # For Thin-plate splines
```
Put the file location of your test set in the first line for test_data, and the training set that we used which you uploaded should be inside of the argument on the second line for data.
```{r}
test_data <- read.csv("D:/School/Cal Poly/S19/STA 5320/Coding Challenge/challenge1testing.csv") 

data <- read.csv("D:/School/Cal Poly/S19/STA 5320/Coding Challenge/challenge1testing.csv")

data <- data[,-1] # X is just an observation number
data$Quality <- as.factor(data$Quality)
data$Style <- as.factor(data$Style)
```

### Function for Test RSS
```{r}
# Typical KNN function
KNN <- function(x,K){
  dist <- (x-data$Sqft)^2
  closest <- order(dist)[1:K]
  yhat <- mean(data$Price[closest])
 
  return(yhat)
}

f <- function(test_set){
  test_set <- test_set[,-1] # X is just an observation number
  test_set$Quality <- as.factor(test_set$Quality)
  test_set$Style <- as.factor(test_set$Style)
  
  sDummy <- test_set$Style == 7
  test_set$sDummy <- sDummy
  
  qDummy <- test_set$Quality == 2
  test_set$qDummy <- qDummy
  
  # -----  Model 1  ----- #
  y <- test_set$Price
  yhat <- predict(backward_model, newdata=test_set)
  loocv <- cv.glm(test_set, backward_model)$delta[1]
  
  cat("-----  Model 1  ----- \n")
  cat("sqrt(LOOCV): ", sqrt(loocv), "\n")
  cat("      LOOCV: ", loocv, "\n\n")
  
  cat("  sqrt(RSS): ", sqrt(sum((y - yhat)^2)), "\n") # sqrt(RSS)
  cat("        RSS: ", sum((y-yhat)^2), "\n\n")
  # cat("  sqrt(MSE): ", sqrt(sum((y - yhat)^2)/nrow(test_set)), "\n") # sqrt(MSE)
  
  # ----- "Model" 2  ----- #
  yhats <- 0
  
  for(i in 1:nrow(test_set)){
    yhats[i] <- KNN(test_set$Sqft[i],2) 
  }
  
  cat("----- \"Model\" 2 ----- \n")
  cat("  sqrt(RSS): ", sqrt(sum((y-yhats)^2)), "\n") # sqrt(RSS), just easier to compare
  cat("        RSS: ", sum((y-yhats)^2))
}
```

### Model 1
```{r}
# Change data to test_set
sDummy <- data$Style == 7
data$sDummy <- sDummy

qDummy <- data$Quality == 2
data$qDummy <- qDummy

backward_model <- glm(Price ~ Sqft + Quality + Lot + Sqft:Bed + Sqft:Year + Year:sDummy + Bath:qDummy, data=data)

summary(backward_model)

# change data to test_set
cat("sqrt(LOOCV): ", sqrt(cv.glm(data, backward_model)$delta[1]))

y <- data$Price # Change data to test_set
yhat <- predict(backward_model, newdata=data) # Replace data with test_set
sqrt(sum((y - yhat)^2)) # sqrt(RSS)

# Replace data with test_set
sqrt(sum((y - yhat)^2)/nrow(data)) # Dividing by observations to get MSE, more comparable for LOOCV.
```

We ended up choosing the model with interaction terms from Backward Selection, with the predictors as stated above. We tried many different linear models, including Best Subset Selection without interaction terms(and tried it with interaction terms, but it wouldn't finish running), linear models with all predictors, Lasso and Ridge both with/without interaction terms, Forward Selection with interaction terms, and more that weren't detailed here. We did not use any polynomial terms as the residuals showed only a slight funnel, addressed in the next paragraph, but the QQ-Plot wasn't exactly perfect although it was not the worst. This model also had the best AIC/BIC, and although the Adjusted $R^2$ for every model isn't amazing, this model seemed to come out on top in all of the areas mentioned. 

The Lasso model without interaction terms was a close second choice, but the LOOCV was convincing enough for the backward selection model to abandon the Lasso one. We did also try to take a log transform because of the slight funnel shape in the residuals, but it didn't seem to do much so we opted out of it in favor of not doing things incorrectly.  

### Model 2
```{r}
KNN <- function(x,K){
  dist <- (x-data$Sqft)^2
  closest <- order(dist)[1:K]
  yhat <- mean(data$Price[closest])
  return(yhat)
}
yhats <- 0
  
# Change both instances of data to test_set for the for loop.
for(i in 1:nrow(data)){
  yhats[i] <- KNN(data$Sqft[i],2) 
}

y <- data$Price # Change data here to test_set
sqrt(sum((y-yhats)^2)) # sqrt(RSS), just easier to compare
```
For the nonparametric methods, we decided on KNN with K=2 and Sqft as our one predictor, as it resulted in the lowest RSS. We looked at using other predictors such Lot, Bath, Bed, Quality, etc. but Sqft had the lowest RSS by a good margin. With regards to Splines vs KNN, we did similar for Splines but saw the best results when still with KNN and K=2, no matter the predictor(comparing to Sqft for KNN). 

Thin Plate Splines were considered but since we could only use one predictor, it didn't really do much for us. We also looked at transforming the data but again the results were not the best and we had extra room for error in doing the transform and exponentiating it back. We didn't want to risk it as our KNN RSS seemed decent as it is. Sorry. :(

This is how you should call the function to get the TestRSS.
```{r}
f(test_data)
```
---
title: "STA 5320 Homework 8"
author: "Kevin Bailey"
date: "3/22/19"
output: html_document
---

# Question 3

```{r}
library(MASS)
library(boot)
library(splines)

kCV <- 0
n <- nrow(Boston)

set.seed(1) # To get same K-fold CV every run.

# Ignore the warnings in the loop.
for(i in 1:20){
  fit <- glm(nox ~ poly(dis, degree=i, raw=TRUE, simple=TRUE), data=Boston)
  kCV[i] <- cv.glm(Boston, fit, K=10)$delta[1]
}
```

### Part (a)
```{r}
cbind(1:20, kCV)
which.min(kCV)
```

As $p$ increases, the 10-fold CV seems to increase for the most part, so for part (f) we need the 4th degree polynomial as m1.
```{r}
deg <- which.min(kCV)
m1 <- lm(nox ~ poly(dis, degree=deg, raw=TRUE, simple=TRUE), data=Boston)
```

### Part (b)

I would say the 4th degree polynomial is the most optimal based solely on this CV? The error is the smallest for that model.

### Part (c)
```{r}
set.seed(1) # To get same K-fold CV every run.

#20-fold CV
# Ignore the warnings in the loop.
for(i in 1:20){
  fit <- glm(nox ~ poly(dis, degree=i, raw=TRUE, simple=TRUE), data=Boston)
  kCV[i] <- cv.glm(Boston, fit, K=20)$delta[1]
}

cbind(1:20, kCV)

#for(i in 1:20){
#  fit <- glm(nox ~ poly(dis, degree=i, raw=TRUE, simple=TRUE), data=Boston)
#  kCV[i] <- cv.glm(Boston, fit, K=506)$delta[1]
#}

cbind(kCV) 
```

Based on the seed I set, the 4th degree polynomial is the most optimal, and the trend does continue throughout.

The more folds we have, the longer the computation takes. For instance, with K=10, we are using 10 folds, so 1/10th of the data as our test set at a time, so we go through 10 iterations. Similarly, for 20 folds, we will have to do twice as many iterations as 10-fold CV.

I did run the LOOCV, which is the last for loop, and the 11th degree polynomial was actually the closest to 0, but only by a slim margin.

### Part (d)
```{r}
nat_cubic_spline <- lm(nox ~ ns(dis, knots=c(2, 4, 6, 8)), data=Boston)
```

### Part (e)
```{r}
# smooth.spline(...) takes the predictor first, then the response second, opposite of how lm(...) does it.
smooth_spline_8 <- smooth.spline(Boston$dis, Boston$nox, df=8)
smooth_spline_cv <- smooth.spline(Boston$dis, Boston$nox, cv=TRUE) # Gives a warning. 
```

### Part (f)
```{r}
# m1 already created and named accordingly in (a)
m2 <- nat_cubic_spline
m3 <- smooth_spline_8
m4 <- smooth_spline_cv

#dis_lims <- range(Boston$dis)
#dis.grid <- seq(from=dis_lims[1], to=dis_lims[2])
dis.grid <- seq(0, 100, by=0.01)

plot(Boston$dis, Boston$nox, col="grey", 
     xlim=c(0,15), ylim=c(0.35,0.9),
     xlab="Distance", ylab="nox")
pred <- predict(m1, newdata=list(dis=dis.grid), se=T)
lines(dis.grid, pred$fit, lwd=2)
lines(dis.grid, pred$fit + 2*pred$se, lty="dashed")
lines(dis.grid, pred$fit - 2*pred$se, lty="dashed")

pred2 <- predict(m2, newdata=list(dis=dis.grid), se=T)
lines(dis.grid, pred2$fit, col="red", lwd=2)

pred3 <- predict(m3, newdata=list(dis=dis.grid), se=T)
lines(m3, col="blue", lwd=2)

pred4 <- predict(m4, newdata=list(dis=dis.grid), se=T)
lines(m4, col="darkgreen", lwd=2)

legend("topright", legend=c("4th degree lm", "natural cubic spline", "8df smoothing spline", "15.43df smoothing spline"), col=c("black","red", "blue", "darkgreen"), lty=1, lwd=2, cex=0.8)
```